{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06aecff",
   "metadata": {},
   "source": [
    "## Roget's Thesaurus\n",
    "\n",
    "This project embarks on an academic exploration of Roget's Thesaurus through computational linguistics, beginning with the meticulous web scraping of the Thesaurus data from the Gutenberg Project. It progresses by harnessing the OpenAI API for advanced word embeddings, with BERT embeddings providing a secondary, comparative perspective. The study then employs unsupervised clustering to investigate the natural grouping of words, aiming to uncover correlations with Roget's original categorizations at both class and section levels. The final phase leverages supervised learning to predict these categorizations, ensuring each step is academically documented for thorough analysis and reproducibility. This concise approach aims to blend traditional lexicography with modern linguistic models, offering new insights into the structure and semantics of language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccddd2",
   "metadata": {},
   "source": [
    "## Get Roget's Thesaurus Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9a144",
   "metadata": {},
   "source": [
    "This script efficiently extracts the hierarchical data from Roget's Thesaurus on the Gutenberg Project website, employing Python's `requests` and `BeautifulSoup` to parse the HTML content. It identifies classes, sections, and subsections by matching specific patterns in anchor tags, and captures words and their synonyms, primarily within bold tags and sibling text nodes. Unwanted characters and punctuation are removed through cleaning functions to ensure data purity. The extracted information is structured into a nested dictionary, reflecting the Thesaurus's organization, and then serialized into a JSON file. This concise approach facilitates a clear understanding of the Thesaurus's complex structure, making the data readily available for computational analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af62505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Fetch the page content\n",
    "url = \"https://www.gutenberg.org/files/10681/old/20040627-10681-h-body-pos.htm#25\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Data structure to hold the hierarchy\n",
    "thesaurus = {}\n",
    "\n",
    "# Apply clean_text to remove unwanted characters from hierarchy names\n",
    "def get_text_excluding_children(tag):\n",
    "    return clean_text(''.join(tag.find_all(text=True, recursive=False)).strip())\n",
    "\n",
    "# Remove unwanted characters from the text, including newlines, special unicode characters, specific punctuation, and the slash character.\"\"\"\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[\\r\\n\\u2020/!\\u00e5]+', '', text).strip()\n",
    "\n",
    "# Split text on punctuation and return a list of cleaned parts, excluding parts with more than two words.\n",
    "def split_on_punctuation(text):\n",
    "    parts = re.split(r'[;,.]', text)\n",
    "    return [clean_text(part) for part in parts if part.strip() and part.strip() != '--' and len(part.split()) <= 2]\n",
    "\n",
    "\n",
    "current_class = None\n",
    "current_section = None\n",
    "current_subsection = None\n",
    "last_valid_word = None \n",
    "synonyms = []\n",
    "\n",
    "# Define regex patterns for precise matching\n",
    "class_title_pattern = re.compile(r'^CLASS')\n",
    "section_title_pattern = re.compile(r'^SECTION')\n",
    "subsection_title_pattern = re.compile(r'^SUBSECTION')\n",
    "pos_pattern = re.compile(r'^(N\\.|V\\.|Adj\\.|Adv\\.|Phr\\.)$')\n",
    "\n",
    "# Extract hierarchy, words, and their synonyms\n",
    "for element in soup.find_all(['a', 'b', 'p']):\n",
    "    # Check for the end element and stop processing if found\n",
    "    if element.name == 'p' and '***END OF THE PROJECT GUTENBERG EBOOK' in element.text:\n",
    "        print(\"End of the Thesaurus detected. Stopping extraction.\")\n",
    "        break\n",
    "    \n",
    "    if element.name == 'a' and 'name' in element.attrs:\n",
    "        name_attr = element['name']\n",
    "        # Process class, section, subsection titles\n",
    "        if class_title_pattern.match(name_attr):\n",
    "            current_class = get_text_excluding_children(element)  # Prepend \"CLASS\"\n",
    "            thesaurus[current_class] = {}\n",
    "            current_section = None\n",
    "            current_subsection = None\n",
    "            last_valid_word = None  # Reset on new class\n",
    "        elif section_title_pattern.match(name_attr):\n",
    "            current_section = get_text_excluding_children(element)  # Prepend \"SECTION\"\n",
    "            thesaurus[current_class].setdefault(current_section, {})\n",
    "            current_subsection = None\n",
    "            last_valid_word = None  # Reset on new section\n",
    "        elif subsection_title_pattern.match(name_attr):\n",
    "            current_subsection = get_text_excluding_children(element)\n",
    "            thesaurus[current_class][current_section].setdefault(current_subsection, {})\n",
    "            last_valid_word = None  # Reset on new subsection\n",
    "\n",
    "    elif element.name == 'b' and not pos_pattern.match(element.get_text().strip()):\n",
    "        # New valid word encountered\n",
    "        last_valid_word = clean_text(element.get_text())\n",
    "        synonyms = []  # Reset synonyms list for the new word\n",
    "\n",
    "    # Check for direct sibling text nodes as synonyms\n",
    "    if last_valid_word:\n",
    "        next_sibling = element.next_sibling\n",
    "        while next_sibling and not isinstance(next_sibling, Tag):\n",
    "            synonym_text = clean_text(str(next_sibling))\n",
    "            if synonym_text:\n",
    "                # Split on punctuation and filter out invalid entries\n",
    "                synonyms.extend(split_on_punctuation(synonym_text))\n",
    "            next_sibling = next_sibling.next_sibling\n",
    "\n",
    "    # Append synonyms to the last valid word in the hierarchy\n",
    "    if last_valid_word and synonyms:\n",
    "        target_dict = thesaurus[current_class][current_section]\n",
    "        if current_subsection:\n",
    "            target_dict = target_dict[current_subsection]\n",
    "\n",
    "        if last_valid_word in target_dict:\n",
    "            target_dict[last_valid_word].extend(synonyms)  # Append new synonyms to existing ones\n",
    "        else:\n",
    "            target_dict[last_valid_word] = synonyms  # Add new word with its synonyms\n",
    "\n",
    "        synonyms = []  # Reset synonyms list after appending\n",
    "\n",
    "# Save data to a JSON file\n",
    "with open('rogets_thesaurus.json', 'w') as outfile:\n",
    "    json.dump(thesaurus, outfile, indent=4)\n",
    "\n",
    "print(\"Thesaurus data extracted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58624b4",
   "metadata": {},
   "source": [
    "To ensure clarity and manageability in our analysis of Roget's Thesaurus, we've decided not to include the detailed divisions within classes 4 and 5, focusing instead on the broader class and section levels. This approach helps maintain the integrity of our data structure and simplifies the analysis, avoiding the potential complications that could arise from the intricate details of these divisions. By consolidating sections within their respective classes, we aim to facilitate a more streamlined and coherent exploration of the Thesaurus's linguistic categorizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff8b9e",
   "metadata": {},
   "source": [
    "## Preprocess for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46d07c",
   "metadata": {},
   "source": [
    "In this script, we load Roget's Thesaurus data from a JSON file and embark on a meticulous process to reconstruct synonyms for each word, emphasizing their interconnectedness. The script introduces a novel approach to encapsulate the commonality between words through the aggregation of synonyms, appending each synonym to its corresponding word to form a composite entity. This is achieved through a series of functions that not only convert class and section identifiers to Roman numerals for a classical touch but also meticulously remove duplicate synonyms to maintain data purity. \n",
    "\n",
    "The `aggregate_synonyms` function stands at the core of this process, ingeniously the synonyms within the hierarchical structure of classes and sections, ensuring a seamless integration of synonyms into the dataset. This reconstructed dataset is then serialized back into JSON format, capturing the enriched lexical relationships, and is saved to a file, ready for embedding analysis. This approach facilitates a deeper semantic analysis, allowing us to explore the nuances of lexical similarity and commonality in a structured and coherent manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c00cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the JSON file\n",
    "with open('rogets_thesaurus.json', 'r') as f:\n",
    "    original_json = json.load(f)\n",
    "\n",
    "# Convert an integer to a Roman numeral.\n",
    "def int_to_roman(input):\n",
    "    if not isinstance(input, type(1)):\n",
    "        raise TypeError(\"expected integer, got %s\" % type(input))\n",
    "    if not 0 < input < 4000:\n",
    "        raise ValueError(\"Argument must be between 1 and 3999\")\n",
    "    ints = (1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1)\n",
    "    nums = ('M', 'CM', 'D', 'CD', 'C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I')\n",
    "    result = []\n",
    "    for i in range(len(ints)):\n",
    "        count = int(input / ints[i])\n",
    "        result.append(nums[i] * count)\n",
    "        input -= ints[i] * count\n",
    "    return ''.join(result)\n",
    "\n",
    "# Remove duplicate synonyms while preserving order.\n",
    "def remove_duplicates(synonyms_list):\n",
    "    seen = set()\n",
    "    deduped_list = []\n",
    "    for item in synonyms_list:\n",
    "        # Convert to lowercase for case-insensitive comparison\n",
    "        if item.lower() not in seen:\n",
    "            deduped_list.append(item)\n",
    "            seen.add(item.lower())\n",
    "    return deduped_list\n",
    "\n",
    "def aggregate_synonyms(data):\n",
    "    new_json = {}\n",
    "    class_counter = 0  # Initialize class counter\n",
    "    for class_title, sections in data.items():\n",
    "        class_counter += 1  # Increment class counter\n",
    "        class_key = f\"CLASS {int_to_roman(class_counter)} {class_title}\"  # Use Roman numeral for class\n",
    "        new_json[class_key] = {}\n",
    "        section_counter = 0  # Reset section counter for each new class\n",
    "        for section_title, content in sections.items():\n",
    "            section_counter += 1  # Increment section counter\n",
    "            section_key = f\"SECTION {int_to_roman(section_counter)} {section_title}\"  # Use Roman numeral for section\n",
    "            new_json[class_key][section_key] = []\n",
    "            if isinstance(content, dict):  # Section has subsections or direct words\n",
    "                for subsection_title, words in content.items():\n",
    "                    if isinstance(words, dict):  # Subsection contains words\n",
    "                        for word, synonyms in words.items():\n",
    "                            # Append the word followed by a space to each synonym, ensuring the synonym is not the same as the word\n",
    "                            modified_synonyms = [syn for syn in synonyms if syn.lower() != word.lower()]\n",
    "                            deduped_synonyms = remove_duplicates(modified_synonyms)\n",
    "                            new_json[class_key][section_key].append(deduped_synonyms)\n",
    "                    else:  # Direct words under section\n",
    "                        for word in words:  # Treat each item as a word with synonyms being the rest of the list\n",
    "                            modified_synonyms = [syn for syn in words if syn.lower() != word.lower()]\n",
    "                            deduped_synonyms = remove_duplicates(modified_synonyms)\n",
    "                            new_json[class_key][section_key].append(deduped_synonyms)\n",
    "            else:  # Direct words under class\n",
    "                for word in content:  # Treat each item as a word with synonyms being the rest of the list\n",
    "                    modified_synonyms = [syn for syn in content if syn.lower() != word.lower()]\n",
    "                    deduped_synonyms = remove_duplicates(modified_synonyms)\n",
    "                    new_json[class_key][section_key].append(deduped_synonyms)\n",
    "    return new_json\n",
    "\n",
    "transformed_json = aggregate_synonyms(original_json)\n",
    "\n",
    "def save_json_to_file(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Specify the filename where you want to save the JSON data\n",
    "filename = 'transformed_data.json'\n",
    "\n",
    "# Call the function to save the JSON data to the specified file\n",
    "save_json_to_file(transformed_json, filename)\n",
    "\n",
    "print(f\"Transformed JSON has been saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d61ec",
   "metadata": {},
   "source": [
    "## Get Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb8d61",
   "metadata": {},
   "source": [
    "This script facilitates the acquisition of word embeddings for Roget's Thesaurus using OpenAI's API, with strategic measures to handle API limitations and interruptions. It commences by loading any previously saved data to avoid redundancy. Then, leveraging the OpenAIEmbeddings class, the script requests embeddings for each word while keeping track of progress. To ensure resilience, it incorporates error handling to manage potential API disruptions. Embeddings are saved incrementally after each section is processed, ensuring the ability to resume without loss of data or unnecessary API calls, thus optimizing both efficiency and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be44a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai_embeddings import process_and_save_embeddings\n",
    "\n",
    "# # Load the data from the JSON file\n",
    "# with open('transformed_data.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# process_and_save_embeddings(data, \"openai.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f21be",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286df7d",
   "metadata": {},
   "source": [
    "In this part of the assignment, we embark on a journey to explore the potential of unsupervised machine learning in mirroring the hierarchical organization of Roget's Thesaurus. Our objective is twofold: first, to group words into their respective classes, and second, to delve deeper by clustering them into sections. This approach allows us to evaluate if the natural clusters formed by the algorithm align with the predefined categories set by Roget, thereby assessing the efficacy of machine learning in capturing semantic relationships inherent in language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d681098",
   "metadata": {},
   "source": [
    "The first step in our analysis involves dimensionality reduction, a crucial process given the high-dimensional nature of word embeddings. By employing UMAP, we aim to condense these embeddings into a 2D space, preserving as much of the original structure as possible. This reduction not only facilitates a more manageable analysis but also enables us to visualize the data, providing intuitive insights into the grouping tendencies of words based on their semantic similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb6394",
   "metadata": {},
   "source": [
    "Following the dimensionality reduction, we apply the K-Means clustering algorithm to the reduced embeddings. The choice of K-Means is motivated by its simplicity and effectiveness in grouping data into distinct clusters. By setting the number of clusters to match the number of classes in Roget's Thesaurus, we seek to discover how well the algorithm can approximate Roget's original classification. This step is pivotal in our exploration, as it directly tests the hypothesis that unsupervised machine learning can effectively replicate human linguistic categorizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76f070",
   "metadata": {},
   "source": [
    "### Class Level Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca9ce4",
   "metadata": {},
   "source": [
    "Please note that the clustering analysis utilizes an existing file with word embeddings that were not generated in the current execution of the script. This file is a product of an older version of the script and due to the time constraints of the assignment, there was insufficient time to regenerate embeddings using OpenAI's API. To maintain the continuity of the project and adhere to the deadline, it was necessary to proceed with the pre-existing embeddings. This approach ensures that the assignment progresses without delay while still providing a valid examination of the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load embeddings and words data\n",
    "with open('openai.json', 'r') as f:\n",
    "    embeddings_data = json.load(f)\n",
    "\n",
    "with open('original.json', 'r') as f:\n",
    "    words_data = json.load(f)\n",
    "\n",
    "word_list, embeddings_list, labels_list, class_names = [], [], [], []\n",
    "label_dict = {}  # To hold the mapping of class names to numeric labels\n",
    "\n",
    "# Iterate over the words data to populate the word list, embeddings list, and labels list\n",
    "for class_index, (class_key, sections) in enumerate(words_data.items()):\n",
    "    label_dict[class_index] = class_key  # Map index to class name for later reference\n",
    "    for section_key, words in sections.items():\n",
    "        for word_group_index, word_group in enumerate(words):\n",
    "            for word in word_group:\n",
    "                word_list.append(word)\n",
    "                class_names.append(class_key)  # Keep track of class names for each word\n",
    "                try:\n",
    "                    embedding = embeddings_data[class_key][section_key][word_group_index][word_group.index(word)]\n",
    "                    embeddings_list.append(embedding)\n",
    "                except IndexError:\n",
    "                    # Handle the error, e.g., log it or skip\n",
    "                    print(f\"IndexError for word: {word} in group: {word_group_index}\")\n",
    "                    continue\n",
    "\n",
    "                labels_list.append(class_index)  # Use class_index as the label\n",
    "\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "true_labels = np.array(labels_list)\n",
    "\n",
    "# Dimensionality reduction with UMAP\n",
    "umap_reducer = UMAP(n_components=2, n_neighbors=15, min_dist=0.0, random_state=42)\n",
    "reduced_embeddings = umap_reducer.fit_transform(embeddings_array)\n",
    "\n",
    "# Clustering with K-Means\n",
    "kmeans = KMeans(n_clusters=len(label_dict), n_init=20, random_state=42)\n",
    "cluster_assignments = kmeans.fit_predict(reduced_embeddings)\n",
    "\n",
    "cluster_names = {}\n",
    "used_names = set()\n",
    "\n",
    "for cluster_id in range(len(label_dict)):\n",
    "    words_in_cluster = [class_names[i] for i, cluster in enumerate(cluster_assignments) if cluster == cluster_id]\n",
    "    common_words = Counter(words_in_cluster).most_common()\n",
    "    \n",
    "    # Find the most common, unused name for the cluster\n",
    "    for name, _ in common_words:\n",
    "        if name not in used_names:\n",
    "            cluster_names[cluster_id] = name\n",
    "            used_names.add(name)\n",
    "            break\n",
    "    else:\n",
    "        # All names are already used, append a count to the most common name\n",
    "        most_common_name, _ = common_words[0]\n",
    "        count = sum(name.startswith(most_common_name) for name in used_names)\n",
    "        unique_name = f\"{most_common_name}_{count}\"\n",
    "        cluster_names[cluster_id] = unique_name\n",
    "        used_names.add(unique_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a0132",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f914894",
   "metadata": {},
   "source": [
    "The first visualization titled \"True Labels in Reduced Dimensional Space\" displays the spread of word embeddings in a two-dimensional space after UMAP reduction, with points color-coded by their true class labels as defined in Roget's Thesaurus. Each color represents one of the six classes of words, ranging from abstract relations to moral powers. \n",
    "\n",
    "The plot shows the natural grouping of words according to their semantic categories, indicating how closely related words cluster together in the reduced space. However, we also observe some overlap between classes, suggesting the complexity of linguistic categories and the nuanced relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with True Labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, class_name in label_dict.items():\n",
    "    idx = np.where(true_labels == label)\n",
    "    plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], label=class_name, s=50, alpha=0.6)\n",
    "plt.title('True Labels in Reduced Dimensional Space')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177018bb",
   "metadata": {},
   "source": [
    "In the second visualization we see the results of the K-Means clustering algorithm applied to the same UMAP-reduced embeddings. Instead of true labels, each point is colored based on the dominant cluster it has been assigned to. The legend uses unique class names for each cluster to provide an intuitive mapping of clusters to Thesaurus categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with Cluster Assignments (using unique class names for legends)\n",
    "plt.figure(figsize=(10, 8))\n",
    "legend_handles = []\n",
    "\n",
    "for cluster_id, cluster_name in cluster_names.items():\n",
    "    idx = np.where(cluster_assignments == cluster_id)\n",
    "    plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], s=50, alpha=0.6)\n",
    "    legend_handles.append(mpatches.Patch(color=plt.cm.viridis(cluster_id / len(cluster_names)), label=cluster_name))\n",
    "\n",
    "plt.title('Clusters in Reduced Dimensional Space')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend(handles=legend_handles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc44c07",
   "metadata": {},
   "source": [
    "The following bar chart represents the proportion of the most common class within each cluster compared to the other classes. The dominance of one class in each cluster could give the impression of a successful clustering effort. However, the high percentage of \"Other Classes\" in each bar indicates that, while one class may be the most frequent in a cluster, a significant portion of the cluster consists of words from different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of the dominant class in each cluster\n",
    "dominant_class_percentage = []\n",
    "for cluster_id in range(len(label_dict)):\n",
    "    words_in_cluster = [class_names[i] for i, cluster in enumerate(cluster_assignments) if cluster == cluster_id]\n",
    "    cluster_size = len(words_in_cluster)\n",
    "    most_common_name, most_common_count = Counter(words_in_cluster).most_common(1)[0]\n",
    "    percentage = (most_common_count / cluster_size) * 100\n",
    "    dominant_class_percentage.append(percentage)\n",
    "\n",
    "# Calculate the rest of the percentage for each cluster\n",
    "other_classes_percentage = [100 - x for x in dominant_class_percentage]\n",
    "\n",
    "# Setting up the bar chart\n",
    "clusters = [f\"Cluster {i+1}\" for i in range(len(label_dict))]  # Cluster labels\n",
    "bar_width = 0.35  # Width of the bars\n",
    "index = np.arange(len(clusters))  # The x locations for the groups\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots()\n",
    "bars1 = ax.bar(index, dominant_class_percentage, bar_width, label='Dominant Class', color='lightblue')\n",
    "bars2 = ax.bar(index, other_classes_percentage, bar_width, bottom=dominant_class_percentage, label='Other Classes', color='lightcoral')\n",
    "\n",
    "# Adding some text for labels, title, and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Clusters')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Percentage of Dominant Class vs Other Classes in Each Cluster')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(clusters)\n",
    "ax.legend()\n",
    "\n",
    "# Displaying the bar chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10afab3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dddeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "silhouette_avg = silhouette_score(reduced_embeddings, cluster_assignments)\n",
    "ari_score = adjusted_rand_score(true_labels, cluster_assignments)\n",
    "nmi_score = normalized_mutual_info_score(true_labels, cluster_assignments)\n",
    "\n",
    "print(f'Silhouette Score: {silhouette_avg:.4f}')\n",
    "print(f'Adjusted Rand Index (ARI): {ari_score:.4f}')\n",
    "print(f'Normalized Mutual Information (NMI): {nmi_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdbfac",
   "metadata": {},
   "source": [
    "* `Silhouette Score` (0.3230): This score ranges from -1 to 1 and measures how similar an object is to its own cluster compared to other clusters. A score of 0.3230 suggests that, on average, clusters are relatively distinct and separate from each other, but there is still considerable room for improvement as the score is closer to 0 than to 1.\n",
    "  \n",
    "\n",
    "* `Adjusted Rand Index (ARI)` (0.0184): The ARI adjusts for the chance grouping of elements and ranges from -1 to 1, where a score close to 0 indicates random labeling, and 1 indicates perfect agreement. An ARI of 0.0184 indicates that there is a very slight agreement between the clustering assignments and the true labels, only marginally better than random chance.\n",
    "  \n",
    "\n",
    "* `Normalized Mutual Information (NMI)` (0.0319): NMI is a normalization of the Mutual Information (MI) score that measures the mutual dependence between the two distributions, ranging from 0 (no mutual information) to 1 (perfect correlation). An NMI of 0.0319 is quite low, suggesting that the clusters found by the algorithm share very little information in common with the true class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc7027",
   "metadata": {},
   "source": [
    "### Section Level Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a146276",
   "metadata": {},
   "source": [
    "In the section-level clustering, the script closely follows the class-level approach, loading word embeddings, assigning unique numerical labels to each section, and aggregating them into lists for analysis. The dimensionality reduction is once again performed using UMAP, reducing the embedding space for enhanced clustering and visualization. K-Means clustering is applied, with the cluster count set to the total number of sections. This streamlined process aims to evaluate whether unsupervised clustering can effectively categorize words into the more granular sections of Roget's Thesaurus, paralleling the methodology used for class-level analysis but with a finer resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "word_list, embeddings_list, labels_list, section_names = [], [], [], []\n",
    "section_label_dict = {}  # To hold the mapping of section names to numeric labels\n",
    "section_counter = 0\n",
    "\n",
    "# Iterate over the words data to populate the word list, embeddings list, and labels list\n",
    "for class_key, sections in words_data.items():\n",
    "    for section_key, words in sections.items():\n",
    "        # Assign a unique integer label for each section\n",
    "        if section_key not in section_label_dict:\n",
    "            section_label_dict[section_key] = section_counter\n",
    "            section_counter += 1\n",
    "        for word_group_index, word_group in enumerate(words):\n",
    "            for word in word_group:\n",
    "                word_list.append(word)\n",
    "                section_names.append(section_key)  # Keep track of section names for each word\n",
    "                embedding = embeddings_data[class_key][section_key][word_group_index][word_group.index(word)]\n",
    "                embeddings_list.append(embedding)\n",
    "                labels_list.append(section_label_dict[section_key])  # Use section index as the label\n",
    "\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "true_labels = np.array(labels_list)\n",
    "\n",
    "# Dimensionality reduction with UMAP\n",
    "umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.0, random_state=42)\n",
    "reduced_embeddings = umap_reducer.fit_transform(embeddings_array)\n",
    "\n",
    "# Clustering with K-Means, using the number of unique sections as the number of clusters\n",
    "kmeans = KMeans(n_clusters=section_counter, n_init=20, random_state=42)\n",
    "cluster_assignments = kmeans.fit_predict(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac03ae",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04038a6",
   "metadata": {},
   "source": [
    "The same dominant class login has been applied here showcaseing the most dominant section in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization without legends\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1],\n",
    "            c=cluster_assignments, cmap='viridis', s=50, alpha=0.6)\n",
    "plt.title('Section Clusters in 2D Reduced Dimensional Space')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28a0ad",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "silhouette_avg = silhouette_score(reduced_embeddings, cluster_assignments)\n",
    "ari_score = adjusted_rand_score(true_labels, cluster_assignments)\n",
    "nmi_score = normalized_mutual_info_score(true_labels, cluster_assignments)\n",
    "\n",
    "print(f'Silhouette Score: {silhouette_avg:.4f}')\n",
    "print(f'Adjusted Rand Index (ARI): {ari_score:.4f}')\n",
    "print(f'Normalized Mutual Information (NMI): {nmi_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e2380",
   "metadata": {},
   "source": [
    "Again we see a relatively  low evaluation metrics for the clustering performed at the section level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687ea69",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ee07f",
   "metadata": {},
   "source": [
    "The visualizations provide a seemingly clear clustering of words from Roget's Thesaurus; however, the dominance of a single class within each cluster belies the true complexity captured by unsupervised machine learning. Despite the appearance of distinct groupings, the presence of multiple other classes or sections within each cluster indicates a less definitive separation than Roget's original classification. This observation highlights the challenge of using unsupervised learning to fully replicate the nuanced structure of human-crafted linguistic categories. Consequently, while the embeddings and clustering techniques offer insightful data representations, they do not achieve classifications entirely comparable to Roget's Thesaurus Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868771c6",
   "metadata": {},
   "source": [
    "## Class Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27427e9e",
   "metadata": {},
   "source": [
    "The prediction segment of this assignment leverages supervised learning to classify words into Roget's Thesaurus classes and sections using word embeddings. The script processes embeddings and labels, employs RandomForestClassifier for its proficiency with high-dimensional data, and adjusts for class imbalances. It features a train-test split for validation and hyperparameter tuning specific to each hierarchical level. The outcome is quantified by accuracy scores and classification reports, offering insights into the model's ability to reflect the thesaurus's structured categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Load embeddings\n",
    "with open('openai.json', 'r') as f:\n",
    "    embeddings_dict = json.load(f)\n",
    "\n",
    "embeddings, class_labels, section_labels = [], [], []\n",
    "\n",
    "# Assuming `embeddings_dict` is your dictionary of embeddings\n",
    "first_non_empty_embedding = next(embedding for class_name, sections in embeddings_dict.items() for section_name, section_embeddings in sections.items() for embedding in section_embeddings if len(embedding) > 0)\n",
    "\n",
    "# Determine the dimensionality of the first non-empty embedding\n",
    "expected_dim = len(first_non_empty_embedding[0])  # Assuming embeddings are lists of lists\n",
    "\n",
    "\n",
    "# Collect embeddings and labels\n",
    "for class_name, sections in embeddings_dict.items():\n",
    "    for section_name, section_embeddings in sections.items():\n",
    "        for embedding in section_embeddings:\n",
    "            embeddings.append(np.mean(embedding, axis=0) if len(embedding) > 0 else np.zeros(expected_dim))\n",
    "            class_labels.append(class_name)\n",
    "            section_labels.append(section_name)\n",
    "\n",
    "X, y_class, y_section = np.array(embeddings), np.array(class_labels), np.array(section_labels)\n",
    "class_le, section_le = LabelEncoder(), LabelEncoder()\n",
    "y_class_encoded, y_section_encoded = class_le.fit_transform(y_class), section_le.fit_transform(y_section)\n",
    "\n",
    "# Split the dataset\n",
    "split_args = {'test_size': 0.2, 'random_state': 42, 'stratify': y_class_encoded}\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y_class_encoded, **split_args)\n",
    "X_train_section, X_test_section, y_train_section, y_test_section = train_test_split(X, y_section_encoded, **split_args)\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, target_names, hyperparameters, scoring_metric='accuracy'):\n",
    "    # Compute class weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    \n",
    "    # Initialize and train the RandomForestClassifier with specified hyperparameters\n",
    "    model = RandomForestClassifier(class_weight=dict(enumerate(class_weights)), **hyperparameters)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovo', average='weighted')\n",
    "    report = classification_report(y_test, y_pred, target_names=target_names, zero_division=1)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\\nROC AUC: {roc_auc}\\nClassification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f706c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for class prediction\n",
    "class_hyperparameters = {'max_depth': 20, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100}\n",
    "\n",
    "# Train and evaluate model for class prediction\n",
    "print(\"Model for Class Prediction:\")\n",
    "train_evaluate_model(X_train_class, X_test_class, y_train_class, y_test_class, class_le.classes_, class_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for section prediction\n",
    "section_hyperparameters = {'max_depth': None, 'max_features': 0.5, 'min_samples_leaf': 4, 'min_samples_split': 6, 'n_estimators': 200}\n",
    "\n",
    "# Train and evaluate model for section prediction\n",
    "print(\"\\nModel for Section Prediction:\")\n",
    "train_evaluate_model(X_train_section, X_test_section, y_train_section, y_test_section, section_le.classes_, section_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62f3ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
